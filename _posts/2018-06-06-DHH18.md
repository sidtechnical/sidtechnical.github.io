---
layout: post
title:  Helsinki Digital Humanities Hackathon 2018.
categories: 
  - blog
tags:
- healthy_internet
published: true
---
<!-- 
Option B: 5 credits, Aalto & HY

 

Public portfolio of your work during the hackathon with added material.

 

1)   Discuss with (or write an email to) your group leader and mikko.tolonen@helsinki.fi, jukka.suomela@aalto.fi and eetu.makela@aalto.fi to get information what extra work you need to do to earn the 5 credits.

2)   Choose a public venue (blog post, google docs etc.) for your portfolio.

3)   Write a documentation of your work during the hackathon (what you did as part of the group) that tells also about your experience of the project (reflect about what you brought to the group, what new did you learn, what you should learn in the future etc.). Length approximately 750-1000 words.

4)   Collect all the relevant documents, information about your groups work as a sensibly documented collection of links (code, presentation, poster) into your portfolio.

5)   Feel free to add whatever comes to mind into your hackathon portfolio.

6)   Send a link to your finished portfolio to mikko.tolonen@helsinki.fi, jukka.suomela@aalto.fi and eetu.makela@aalto.fi and Mikko and Jukka will grant you the 5 credits. -->

I stereotype most of the hackathons as a weekend event for computer nerds, where technology companies are pushing their newly released products to be tested or to seek innovative usecases. Typically, such hackathons end up being cheap labour for companies where they get their work done quickly from the the participants who are there for the sake of "free pizzas and beers", or at most, for the glory of winning a hoodie. Recently, I attended the fourth edition of [Helsinki Digital Humanities Hackathon (#DHH18)][dhh18] which was intruitingly different from most of the other hackathons I have attended so far.

---

## 1. What is it all about?

This one week (23/05/18 - 01/06/18) hackathon in the University of Helsinki brought together students and researchers of humanities, social sciences and computer science under the same hood of digital humanities. As per their definition, Digital humanities is "*applying modern data processing to solve research questions in the humanities and social sciences*". But, to most of the participants, it was more of a networking event while working on multidisciplinary **research** project. 

Yes! There was no competition to win prizes, instead, a pure enthusiasm to learn something new was clearly evident.

There were five themes ( = teams) working on different interesting societal topics which would be complicated without computational automation as follows :
 - *People in the News* - exploring the newsworthiness of people found in National Library of Finland’s newspaper corpus.
 - *Rus­sia Fin­land* - to find how these two countries portray each other in their news media.
 - *Early Mod­ern Pub­lish­ing* - interpreting the developments publication practices, genres, and roles of publishers from the 18th century.
 - *The Death Psalm of Bishop Henry* - extracting historical contents from multiple versions of orally transmitted literature Lalli who killed the Bishop Henry as per the legends.
 - *Hel­sinki in Geot­agged So­cial Me­dia* - utilizing the social media as a source of data for understanding Helsinki in terms of its cultural diversity.

May be because of my experience with social media as a social media addict ( i.e. partially forced due to societal norms), I was chosen to be in the Hel­sinki in **Geo**t­agged **So**­cial **Me**­dia (_GeoSoMe_) team. 

 Our awesome geosome team was as bellow:
 - Firaz, an architect turned humanitarian
 - Saara, the linguist
 - Anton and Lulia, the two young hackers from ITMO university of Russia
 - Primary mentor: Asst. Prof. [Tuomo Hippala][tuomo], an expert in multimodal communication along with many other things.  
 - [Prof. Tuuli Toivonen] [tulli] (geoinformatics) was the other mentor.
 - [Simon][simon] and Elias to assist us with the servers and geoinformatics tools respectively.

---

## 2. What we had and what was our idea?

On the first day of the hackathon, Dr. Tuomo gave an introductory presentation about the research that he is doing along with Tuuli in the [Digital Geography Lab][digigeo]. We had the option to work on one or more of the following datasets:


<table class="tablesaw tablesaw-stack" data-tablesaw-mode="stack"  align="center" border="1">
		<tbody>
			<tr>
				<th colspan="1" rowspan="1">
					Dataset
				</th>
				<th colspan="1" rowspan="1">
					# Records
				</th>
				<th colspan="1" rowspan="1">
					# Users
				</th>
					<th colspan="1" rowspan="1">
					Temporal extent
				</th>
				<th colspan="1" rowspan="1">
					Spatial extent
				</th>
			</tr>
			<tr>
				<td colspan="1" rowspan="1">
					<p><span>Flickr</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>126017</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>2701</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>01/01/14 - 07/10/17</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>Helsinki Region</span></p>
				</td>				
			</tr>
			<tr>
				<td colspan="1" rowspan="1">
					<p><span>Instagram</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>1316705</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>207329</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>01/06/14 - 31/03/16</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>Helsinki Region</span></p>
				</td>				
			</tr>
			<tr>
				<td colspan="1" rowspan="1">
					<p><span>Twitter</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>61338</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>9260</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>17/02/16 - 02/03/18</span></p>
				</td>
				<td colspan="1" rowspan="1">
					<p><span>Helsinki Municipality Region</span></p>
				</td>				
			</tr>

</tbody>
	</table>

<p align="center">
    <em>Table 1: Dataset available to the GeoSoMe team at #DHH18</em>
</p>


During the first three days of preparation, our team decided to work on the Instgram dataset, simply because it was the largest dataset we had and it was interesting. Also, we wanted to perform some sentiment analaysis of all aspects the instagram post - captions, hashtags and emojis (and if time permits, the images as well), to see what areas of Helsinki metropolitan region is the happiest. We even boldly presented [our plan][1stpresentation] for the hackthon as **"Happiness in social media"** at the end of the preparation phase (first three days). Here is how our data looked if we simply plotted on a map without any processing:

![Figure 1: Instgram dataset overview]({{site.baseurl}}/assets/images/dhh_dataset.gif)
<p align="center">
    <em>Figure 1: Instagram dataset on a map: Growth of Instagram posts over time</em>
</p>

Over the next couple of days, we figured out that defining happiness and quantifying it over social media is not a low hanging fruit. Especially because when there is the happiness bias, i.e. everyone pretends to be positive over social media as happy people, whatever results that we can produce in one week would be misleading. Furthermore, sentiment analysis to computers is just *positive (+ve)*, *negative (-ve)* or *neutral (0)*, whereas in reality the whole notion of **human sentiments or emotions**<span data-balloon-length="large" data-balloon="Plutchik-wheel: Contrasting and categorization of emotions" data-balloon-pos="up">[<sup><i class="fa fa-wikipedia-w" aria-hidden="true"></i></sup>][real_sentiments]</span> stretches over at least eight dimensions such as *love, optimism, submission, awe, disapproval, remorse, contempt* and *agressiveness*. So, eventually (to be honest, after the [midway presentation][2ndpresentation]) we narrowed down from over-enthusiastic "happiness" to a more realistic research questions on sentiments as following two explorations:
 - **Spatial**: How sentiment polarity is distributed in the neighborhoods of Helsinki?
 - **Temporal**: What is the variation of sentiments over time?

_**My role**_: Being one of the three team members who can code, I took responsibilities of cleaning the data, followed by running language detection algorithms, and doing some traditonal natural language processing tasks before delivering it to be plotted on a map.

---

## 3. Pre-processing

While most of the other teams had clear objectives, ours was quiet vague. However, the actual problem was to deal with the messy data that was inherently cluttered, multilingual, and contain contents (text, emojis, hashtags) that could be completeley random and not neceserrily compliment each other. 

Since core of our idea was to perform sentiment analysis, we used the following pre-processing strategies:
 1. Remove all posts with no **good text content** by dropping posts with no texts or contains just emojis and hashtags.
 2. Restrict the posts that are only in **English**, because sentiment analysis for texts in languages other than English yields no good results at all.
 3. Filter the posts that are in **Helsinki metropolitan region** only. In other words, remove every post that was geotagged within Espoo or Vaanta region.

 The first task was quiet simple. As I learned, removing posts with no **text** by dropping posts with no text can be done in two-liner code when loading the data into [Pandas][pandas] dataframe as follows:
```
df = pd.read_csv("instagram.tsv", na_values=['', ' ', '\N'], sep="\t")
 df = df.applymap(lambda x: np.nan if isinstance(x, basestring) and x.isspace() else x)
```
Basically, convert `caption` cells with empty spaces to `Nan` values with `na_values` argument of `pandas.read_csv()` function and then drop them.

Now for the **language detection**, the available Python libraries were [langdetect][langdetect], [langid][langid], [NLTK][nltk] and [fastText][fastText]. We chose **`fastText`**, because it had pre-trained language identification models for 176 languages; it was fast and reliable. As the state-of-the-art library by Facebook Research, it was suitable for dealing with mulilingual social media platforms like Instagram.

It is very common for people to use multiple languages while posting on social media. However, we wanted posts that are genuinely in English. So, using `fastText` we firsttagged each posts with a language to represent is as a [**language  diversity treemap**]({{site.baseurl}}/assets/images/dhh_Language_Diversity_treemap.png){:target="_blank" _}. We omitted all those posts that were tagged with multiple languages, and retianed the English posted which has a `confidence score` of **more than 70%**.

Then to restirct the posts within Helsinki metropolitan region, we applied [custom **`neighborhoods polygon`** map of Helsinki] and left all other points outside it. To divide Helsinki into discernible units, we could have also relied on `Postcode division`, `Square grids` or `Land use division` criteria.

<i class="fa fa-print fa-2x fa-pull-left fa-border" aria-hidden="true"></i> At this point, we had reduced **1,316,705** Instagram posts from our initial dataset to **193,111** posts that are in English and within "Helsinki". 

---


## 3. Sentiment analysis

Used tools: 
 - VADER (analyze clear text without hashtags and emojis)
 - Aylien API (analyze whole captions)
- Checked against manually annotated gold standard.

Filtering results: set threshold of polarity confidence to 0.7 

Obstacles: hashtags are inserted into sentences and should be considered as their integrated part


![Figure 1: Comparison of sentiment analysis APIs]({{site.baseurl}}/assets/images/dhh_sentiment_comparison.png)
<p align="center">
    <em>Figure 2: Efficiency of sentiment analysis APIs (VADER vs Aylien) with manually annotated goal standard</em>
</p>

## 4. Results

Here are the end product "Sentiments in Helsinki - Spatiotemporal Analysis of Instagram Posts"

![Figure 1: Comparison of sentiment analysis APIs]({{site.baseurl}}/assets/images/dhh_density_posts_map.svg)
![Figure 1: Comparison of sentiment analysis APIs]({{site.baseurl}}/assets/images/dhh_dominant_sentiment.svg)
![Figure 1: Comparison of sentiment analysis APIs]({{site.baseurl}}/assets/images/dhh_season_map.svg)
![Figure 1: Comparison of sentiment analysis APIs]({{site.baseurl}}/assets/images/dhh_temporal_sentiment_polarity.svg)

[dhh18]: https://www.helsinki.fi/en/helsinki-centre-for-digital-humanities/helsinki-digital-humanities-hackathon-2018-dhh18
[tuomo]:http://www.helsinki.fi/~thiippal/
[tulli]: https://tuhat.helsinki.fi/portal/en/person/tutoivon
[simon]:http://hengchen.net/
[digigeo]: https://www.helsinki.fi/en/researchgroups/digital-geography-lab
[1stpresentation]: https://docs.google.com/presentation/d/1EcJg3lqM8iFnrP0bDpjnKKYMYNBPMNkYj6pJ74A4MHI/edit#slide=id.p
[real_sentiments]: https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions#/media/File:Plutchik-wheel.svg
[2ndpresentation]: https://docs.google.com/presentation/d/1C-lHOI8Duui2To7hrov7pgYqxzVcjrGg718XS5onoVg/edit#slide=id.p
[pandas]: https://pandas.pydata.org/
[langdetect]:https://pypi.org/project/langdetect/
[langid]:https://github.com/saffsd/langid.py
[nltk]: https://www.nltk.org/
[fastText]: https://github.com/facebookresearch/fastText
[vader]: https://github.com/cjhutto/vaderSentiment
[aylien]:https://aylien.com/text-api/sentiment-analysis/
